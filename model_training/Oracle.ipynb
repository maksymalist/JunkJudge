{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"MirFPgq_fKhm"},"outputs":[],"source":["!unzip drive/MyDrive/datasets/garbage_types_xl.zip -d ./data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jzt7LNoSUMdS"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torchvision\n","from torchvision import models\n","from torchvision.models import ConvNeXt_Tiny_Weights\n","from torch.utils.data import DataLoader\n","from torch.utils.data import Dataset\n","import torch.optim as optim\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","import matplotlib.pyplot as plt\n","import torchvision.transforms as transforms\n","from PIL import Image\n","import os"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hyePjveLgEtj"},"outputs":[],"source":["DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","CLASSES_1 = {'biological': 0, 'cardboard': 1, 'glass': 2, 'metal': 3, 'paper': 4, 'plastic': 5, 'trash': 6} # XL model for bio + trash\n","CLASSES_2 = {'cardboard': 0, 'glass': 1, 'metal': 2, 'paper': 3, 'plastic': 4, 'trash': 5} # regular model for everything else\n","MODEL1_PATH = \"./models/neo.pth\"\n","MODEL2_PATH = \"./models/trinity.pth\"\n","DATASET_SAVE_PATH = \"./drive/MyDrive/datasets/prediction_model_softmax.csv\"\n","\n","MASTER_INDEX = {\n","    'biological1': 0, \n","    'cardboard1': 1, \n","    'glass1': 2, \n","    'metal1': 3, \n","    'paper1': 4, \n","    'plastic1': 5, \n","    'trash1': 6,\n","    'cardboard2': 7, \n","    'glass2': 8, \n","    'metal2': 9, \n","    'paper2': 10, \n","    'plastic2': 11, \n","    'trash2': 12\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6A2TqqUMf6xV"},"outputs":[],"source":["def CONV_NN(num_classes):\n","    \n","    # load the pretrained model because what's the point of re-inventing the wheel??\n","    model = models.convnext_tiny(weights=ConvNeXt_Tiny_Weights.DEFAULT) # in the new version of PyTorch, you can't use pretrained=True\n","    \n","    # Freeze all the layers because we only want to train the new layers\n","    \n","    for param in model.parameters():\n","        # disable gradients because we don't need to include pretrained data in the backprop\n","        param.requires_grad = False\n","        \n","        \n","    # Replace the last layer with a MLP mixer with dropout\n","    \n","    model.classifier[-1] = nn.Sequential(\n","        nn.Linear(768, 256),\n","        nn.ReLU(),\n","        nn.Dropout(0.5),\n","        nn.Linear(256, num_classes)\n","    )\n","    \n","    return model"]},{"cell_type":"markdown","metadata":{"id":"rAGX3PMlgKCc"},"source":["# The models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MLzB1F3KgI8b"},"outputs":[],"source":["model_1 = CONV_NN(len(CLASSES_1))\n","model_1.to(DEVICE)\n","model_1.load_state_dict(torch.load(MODEL1_PATH, map_location=torch.device('cpu')))\n","\n","model_2 = CONV_NN(len(CLASSES_2))\n","model_2.to(DEVICE)\n","model_2.load_state_dict(torch.load(MODEL2_PATH, map_location=torch.device('cpu')))"]},{"cell_type":"markdown","metadata":{"id":"NfwrJfkbgjD8"},"source":["# Create dataloader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V0C1DpSXgnZk"},"outputs":[],"source":["transform = transforms.Compose([\n","    transforms.Resize((224, 224)), # resize image\n","    transforms.CenterCrop((224, 224)), # center crop image\n","    transforms.ToTensor(), # convert to Pytorch tensor\n","    transforms.Normalize((0.485, 0.456, 0.406),(0.229,0.224,0.225)) # normalize\n","])\n","\n","original_dataset = torchvision.datasets.ImageFolder(\n","    root='./data/Garbage Classification',\n","    transform=transform\n",")\n","\n","print(len(original_dataset))"]},{"cell_type":"markdown","metadata":{"id":"P6rzFnUOhkNh"},"source":["# Utils for the dataset creation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aA36CRAThZIo"},"outputs":[],"source":["def map_classes(predictions, cls):\n","    output = {}\n","    keys = list(cls.keys())\n","\n","    for i in range(len(predictions.squeeze())):\n","        output[list(keys)[i]] = predictions[0][i].item()\n","        \n","    return output\n","\n","def get_predictions(img):\n","    pred1 = model_1(img.unsqueeze(0).to(DEVICE))\n","    pred2 = model_2(img.unsqueeze(0).to(DEVICE))\n","\n","    softmax = nn.Softmax(dim=1)\n","    softmax1 = softmax(pred1)\n","    softmax2 = softmax(pred2)\n","            \n","    c1 = map_classes(softmax1, CLASSES_1)\n","    c2 = map_classes(softmax2, CLASSES_2)\n","\n","    return c1, c2\n","\n","def preds_to_data(c1, c2):\n","    \n","    data = torch.zeros(13)\n","    \n","    for key in c1.keys():\n","        new_key = str(key+\"1\")\n","        data[MASTER_INDEX[new_key]] = c1[str(key)]\n","        \n","    for key in c2.keys():\n","        new_key = str(key+\"2\")\n","        data[MASTER_INDEX[new_key]] = c2[str(key)]\n","        \n","    return data\n","\n","def save_csv(pred_data, label_data):\n","  print(\"saving ...\")\n","  data = {\"prediction\": pred_data, \"label\": label_data}\n","  df = pd.DataFrame(data)\n","  df.to_csv(DATASET_SAVE_PATH, index=False)"]},{"cell_type":"markdown","metadata":{"id":"mRaI5C6ph3Pv"},"source":["# Predict the images and store the preds with the labels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LG7fB3rwiBRK"},"outputs":[],"source":["pred_col = []\n","label_col = []\n","\n","for idx, (image, label) in enumerate(original_dataset):\n","  image = image.to(DEVICE)\n","\n","  c1, c2 = get_predictions(image)\n","  data = preds_to_data(c1, c2)\n","\n","  pred_col.append(data.detach().cpu().numpy().tolist())\n","  label_col.append(label)\n","\n","  print(idx)\n","\n","  # checkpoint system\n","  if idx % 100 == 0:\n","    save_csv(pred_col, label_col)\n","\n","    print(\"###########################################################\")\n","    print(f\"##     Effective: {idx+1}/{len(original_dataset)}       ##\")\n","    print(\"############################################################\")\n","\n","save_csv(pred_col, label_col)\n","  "]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMEAtxEuA8xrpaqVOXCuEWR","mount_file_id":"17Q1HoISeTV1rLWOKb5STre53ORHxh32H","private_outputs":true,"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
